{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lecture 1 The Learning Problem\n",
    "##### Terms and notations\n",
    "***{} - Set, F - function, V - vector, R - real number, I - integer, () - Tuple***\n",
    "\n",
    "- $f$ - F - target function\n",
    "- $h$ - F - hypothesis function\n",
    "- $g$ - F - final hypothesis function\n",
    "- $(x, y)$ - () - training data\n",
    "- $x$ - V - training data input features\n",
    "- $w$ - V - weights in neural network/ parameters in regression\n",
    "- $y$ - I - training data labels\n",
    "- $H$ - {F} - hypothesis set\n",
    "- $N$ - {(V, I)} - training data set\n",
    "- $X$ - {V} - training data inputs\n",
    "- $Y$ - {I} - training data labels\n",
    "\n",
    "---\n",
    "### Questions to answer\n",
    "***[<] partially answered, [=] good answer, [~] fair answer***\n",
    "- When learning is used?[=]\n",
    "- What is the learning diagram?[<]\n",
    "- What is perceptron learning algorithm?[<]\n",
    "- What is the basic learning types?[~]\n",
    "\n",
    "---\n",
    "### 1. When Learning Is Used?\n",
    "#### A pattern exists\n",
    "Nice to have, we can run the machine learning to find out if there is a pattern to learn.\n",
    "#### We cannot pin down it mathematically\n",
    "Nice to have, if we can pin down the problem mathematically, we just need to implement the target function directly instead of finding the $g\\approx f$.\n",
    "#### We have data\n",
    "Must have, if no enough data, we cannot create a final hypothesis function to generalize the target function. Topic of lecture 2 \n",
    "\n",
    "### 2. Components of learning\n",
    "##### Learning Algorithm\n",
    "1.Take in traning data $N$\n",
    "\n",
    "2.Create a model that generate the H(hypothesis set)\n",
    "\n",
    "3.Output best $g$, best means $g(X) - f(X) \\approx 0$.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/components-of-learning.png\" />\n",
    "\n",
    "### 3. Binary Perceptron Learning Algorithm\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/PLA.png\" />\n",
    "\n",
    "### 4. Basic Learning Types\n",
    "##### Supervised Learning\n",
    "$(X, Y) \\Rightarrow g\\approx f$\n",
    "#### Unsupervised Learning\n",
    "$(X, ?)$, $g$ should find out the inner relationship of the input data. ? means unknow relationship between inputs. If later on knows the meaning of ?, we can classify the input data, or regression the data, but I don't think regression problem would be much useful to start with unsupervised learning. If not meaning on it's own we still might find out the meaning after the inputs is been clustered.\n",
    "\n",
    "#### Reinforcement Learning\n",
    "$(X,$ grade output from $h)$, **Note too much on the topic yet\n",
    "\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/basic-learning-types.png\" />\n",
    "\n",
    "---\n",
    "### Math background\n",
    "##### Vector transpose ($w^T$) [link](https://www.khanacademy.org/math/linear-algebra/matrix-transformations/matrix-transpose/v/linear-algebra-transpose-of-a-vector)\n",
    "vector w transpose times vector x equal to number y ($w^Tx$ = y)\n",
    "\n",
    "#### Understanding the Dot Product of Vectors [link](http://www.mvps.org/DirectX/articles/math/dot/index.htm) [khan proof](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/linear-algebra-vector-triangle-inequality)\n",
    "\n",
    "#### Understanding The Law of Cosines\n",
    "Help you to understand $w^Tx$ > 0 or $w^Tx$ < 0\n",
    "\n",
    "$c^2 = a^2 + b^2 âˆ’ 2ab\\cos(C) \\Rightarrow \\dfrac{c^2 - (a^2 + b^2)}{-2\\cos(C)} = ab \\Rightarrow \\text{if } C > 90^{\\circ} \\text{, then } ab < 0 \\text{, otherwise } ab > 0$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lecture 2 Is Learning Feasible?\n",
    "##### Terms and notations\n",
    "***{} - Set, F - function, V - vector, R - real number, I - integer, () - Tuple***\n",
    "\n",
    "- $\\mathit{E}_{in}(h)$ - F(F) - in-sample error\n",
    "- $\\mathit{E}_{out}(h)$ - F(F) - out-sample error\n",
    "\n",
    "---\n",
    "### Questions to answer\n",
    "***[<] partially answered, [=] good answer, [~] fair answer***\n",
    "- How to tell if learning is feasible?[<]\n",
    "\n",
    "---\n",
    "### 1. Learning the target function is impossible\n",
    "Meaning unknow target function f\n",
    "### 2. Hoeffding's Inequality\n",
    "***In order to use hoeffding's Inequality, we need to get X from any distribution P(X) ***\n",
    "Since we don't know the target function f, we cannot know the error rate for g in out-sample, but with hoeffding's inequality, we can bound $\\mathit{E}_{out}(g) - \\mathit{E}_{in}(g) \\geq \\epsilon$ to a small probability by giving big sample set N and proper $\\epsilon$\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/hoeffdings-inequality.png\" />\n",
    "\n",
    "We are trying to <b>learn</b> g$\\approx$f, means we are not only dealing with one hypothesis function h, but a set of them, H. Hoeffding's inequality only work for single case, if we apply it multi-times the probability of miss represent will greater than $\\epsilon$, so we need union bound for all h in H. M here is the number of hyphothesis in H. <b>M is determined by the learning algorithm model, bigger M looser the link between $\\mathit{E}_in(g)$ and $\\mathit{E}_out(g)$, means g might not able to generalize f</b>\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/the-final-verdict.png\" />\n",
    "### 3. What Does Learning Is Feasible Mean?\n",
    "1. Learning is feasible means base on training data to generalize. By giving trainning data N big enough, number of hypothesis M small enough and $\\epsilon$ reasonable enough, In-sample error can track out-sample $|\\mathit{E}_in(g) - \\mathit{E}_out(g)| < \\epsilon$, then we would know that our final hypothesis g can generalize.\n",
    "2. Learning is feasible means learning algorithm is able to pick one best g from H that has small out-simple error.\n",
    "\n",
    "---\n",
    "### Math background\n",
    "##### Hoeffding's Inequality\n",
    "This inequality is very important to machine learning, but I don't think we need to know the proof, I looked Wiki, the proof is complicated for me to understand, so I decide to move on and just going to understand the ideas it is used in machine learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lecture 3 The Linear Model 1\n",
    "##### Terms and notations\n",
    "\n",
    "---\n",
    "### Questions to answer\n",
    "***[<] partially answered, [=] good answer, [~] fair answer***\n",
    "- What is linear regression?[~]\n",
    "- What is the relationship between hypothesis function and cost function?[~]\n",
    "- How to minimizing cost function of linear regression?[~]\n",
    "- How to convert a regression problem to a classification problem?[~]\n",
    "- How nonlinear transformation of X is going to help linear regression problem?[~]\n",
    "\n",
    "---\n",
    "### 1. Input Representation\n",
    "Abstruct features is essensial for learning algorithm to create g, useless features will be noises and learning algorithm will fail to generalize.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/input-representation.png\" />\n",
    "\n",
    "### 2. The 'Pocket' Learning Algorithm\n",
    "It's a variation of PLA, it will keep the best hypothesis which has small in-sample error as the final hypothesis for Linear non-separable data.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/pocket-algorithm.png\" />\n",
    "\n",
    "### 3. Measure In-Sample Error For Real-Vauled Output\n",
    "We was talking about in-sample error $\\mathit{E}_in(h)$, but how to measure it? use $(h(x) - f(x))^2$, so called squared error, here x is a vector of all features, both outcomes from h(x) and f(x) are real-valued number. Since our h function is not only for 1 sample, so we need calculate the mean of squared error for all samples. N is number of samples, X is all samples, y is the vector of known outcomes from unknow target function f. w is the vector of weights for features.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/measure-the-error.png\" />\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/measure-the-error2.png\" />\n",
    "\n",
    "### 4. Minimizing $\\mathit{E}_in(h)$\n",
    "We are trying to minimizing the error of h, since h(X) = $Xw$ and we don't have control over X, so we need use linear regression to find out best weights vector w for all sample X to produce outcome close to f(X). Here we are using matrix calculus to get all the minizied value, I think it's very similar to multi-variable calculus, so I just provide the cheatsheet in mathbackground, we just need to know the idea that by calculate $\\frac{dh}{d\\vec{\\mathbf{w}}} = 0$, we get minimun error value.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/minimizing-error.png\" />\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/linear-regression.png\" />\n",
    "\n",
    "### 5. Linear Regression Classification\n",
    "We can use linear regression to calculate initial weights for binary classification.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/linear-regression-classification.png\" />\n",
    "\n",
    "### 6. Nonlinear Transformation\n",
    "Remember we don't have control over X, the only variable we can manipulate is the vector w in h(X). so we can treat X as constant, and do some nonlinear transformation, this won't change the linearity of w,then apply linear regression to w.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/nonlinear-ransformation.png\" />\n",
    "\n",
    "\n",
    "### Summary\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/the-linear-model-1-summary.png\" />\n",
    "\n",
    "---\n",
    "### Math background\n",
    "##### Matrix calculus cheatsheet\n",
    "<img width=\"250px\" height=\"250px\" src=\"https://i.stack.imgur.com/apR2q.png\" />\n",
    "\n",
    "##### Linear regression [khan](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/residuals-least-squares-rsquared/a/introduction-to-residuals)<br>\n",
    "Great explaination on linear regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lecture 4 Error and Noise\n",
    "##### Terms and notations\n",
    "- $E(h,f)$ - F(F, F) - Error measure function\n",
    "- $E_x(x)$ - R/V - Expected value of x\n",
    "\n",
    "---\n",
    "### Questions to answer\n",
    "***[<] partially answered, [=] good answer, [~] fair answer***\n",
    "- What the error we measures?[~]\n",
    "- How does error measure related with machine learning?[=]\n",
    "- How to choose the error measure function?[~]\n",
    "- What is noise?[~]\n",
    "- How does do we reduce the noise of input data?\n",
    "- What is the supervise learning diagram?[=]\n",
    "\n",
    "\n",
    "---\n",
    "### 1. Error Measures\n",
    "Error measure is used to validate how well our h model the data by any cost function e(h(x), f(x)) on any point, ex: square error function or logistic error function. If the cost is small, h approximates f well on that point.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/error-measures.png\" />\n",
    "We want h approximates f well on all the data in domain, E(h, f), so we need take the average e(h(x), f(x)) on all the points.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/pointwise-error-measure-to-all.png\" />\n",
    "\n",
    "### 2. How to choose the error measure\n",
    "What error function to use? It's not a analytic question, it'a domain question. Depends on the user requirement.\n",
    "But most of the time user do not know the error measure model, so we should choose either plausable error measure or friendly error measure, ex: square error function, cross entropy error function.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/cia-error-measure-ex.png\" />\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/error-measure-conclusion.png\" />\n",
    "\n",
    "### 3. Learning diagram with error measure\n",
    "Error measure quantifies the performance of g, and learning algorithem can use error measure function to minimize the error. The error we measure here is in sample error, we will leverage generalization theory to \"measure\" the out sample error for g\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/learning-diagram-with-error-measure.png\" />\n",
    "\n",
    "### 4. Noisy targets\n",
    "Noise, in practical, is that sometimes the target function will generate different outcomes by giving same input data. The target function is no longer a function, because with noise, it is going to be the distrubution $P(y\\mid x)$.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/target-distribution.png\" />\n",
    "\n",
    "### 5. The complete diagram for supervise learning\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/supervise-learning-diagram.png\" />\n",
    "\n",
    "### 6. Distinction between $P(y\\mid x)$ and $P(x)$\n",
    "The goal of machine learning is trying to learn the unknow target distribution $P(y|x)$. We are given input distribution $P(x)$, we are not going to learn the input distribution. But I think it's important that the training data is generate from the unknow input distribution. otherwise the in-sample error is not going to track out-sample well.\n",
    "<img width=\"800px\" height=\"800px\" src=\"./resources/diff-between-pyx-px.png\" />\n",
    "\n",
    "### 7. Preamble of the theory\n",
    "$E_{out}(g) \\approx 0$ is the goal of g of machine learning. But we can never no the performance of $E_{out}(g)$ directly, so we need to have make $E_{out}(g) \\approx E_{in}(g)$ and $E_{in}(g) \\approx 0$, in order to achieve the goal. ***Sometimes through the proxy of in-sample error, we cannot make out-sample error to 0, but the consistent track of out-sample error from in-sample error sometimes is more important than minimize in-sample error.***\n",
    "<img width=\"800px\" height=\"800px\" src=\"./resources/preamble-of-the-theory.png\" />\n",
    "<img width=\"500px\" height=\"800px\" src=\"./resources/theory-will-achieve.png\" />\n",
    "\n",
    "\n",
    "---\n",
    "### Math background\n",
    "##### Expected value [wiki](https://en.wikipedia.org/wiki/Expected_value)\n",
    "$E$, In probability theory, the expected value of a random variable, intuitively, is the long-run average value of repetitions of the experiment it represents.\n",
    "\n",
    "##### random variable [wiki](https://en.wikipedia.org/wiki/Random_variable) [link](https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/discrete-and-continuous-random-variables/v/random-variables)\n",
    "Here the value is the error cost.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Terms and Notations\n",
    "\n",
    "### target function\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../markdown-plus/lecture-notes.css\", \"r\").read()\n",
    "    return HTML(\"<style>\" + styles + \"/<style>\")\n",
    "#css_styling() not github friendly\n",
    "#css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
