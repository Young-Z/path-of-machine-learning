{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lecture 1 The Learning Problem\n",
    "##### Terms\n",
    "[target function](#target-function),&nbsp;&nbsp;[$f$](#target-function),&nbsp;&nbsp;[hypothesis function](#hypothsis-function),&nbsp;&nbsp;[$h$](#hypothsis-function),&nbsp;&nbsp;[final hypothesis function](#final-hypothsis-function),&nbsp;&nbsp;[$g$](#final-hypothsis-function),&nbsp;&nbsp;[traning data set](#traning-data-set),&nbsp;&nbsp;[$N$](#traning-data-set),&nbsp;&nbsp;[$X$](#all-possible-data-set),&nbsp;&nbsp;[hypothesis set](#hypothesis-set),&nbsp;&nbsp;[$H$](#hypothesis-set),&nbsp;&nbsp;[$x$](#training-data),&nbsp;&nbsp;[$w$](#weights),&nbsp;&nbsp;[$y$](#training-data-outcome),&nbsp;&nbsp;[$Y$](#training-data-outcome-set)\n",
    "\n",
    "---\n",
    "### 1. When Learning Is Used?\n",
    "#### A pattern exists\n",
    "Nice to have, we can run the machine learning to find out if there is a pattern to learn.\n",
    "#### We cannot pin down it mathematically\n",
    "Nice to have, if we can pin down the problem mathematically, we just need to implement the target function directly instead of finding the $g\\approx f$.\n",
    "#### We have data\n",
    "Must have, if no enough data, we cannot create a final hypothesis function to generalize the target function. Topic of lecture 2 \n",
    "\n",
    "### 2. Components of learning\n",
    "##### Learning Algorithm\n",
    "1.Take in traning data $N$\n",
    "\n",
    "2.Create a model that generate the H(hypothesis set)\n",
    "\n",
    "3.Output best $g$, best means $g(X) - f(X) \\approx 0$.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/components-of-learning.png\" />\n",
    "\n",
    "### 3. Binary Perceptron Learning Algorithm\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/PLA.png\" />\n",
    "\n",
    "### 4. Basic Learning Types\n",
    "##### Supervised Learning\n",
    "$(X, Y) \\Rightarrow g\\approx f$\n",
    "#### Unsupervised Learning\n",
    "$(X, ?)$, $g$ should find out the inner relationship of the input data, later on if knows $y$, we can classify the input data\n",
    "#### Reinforcement Learning\n",
    "$(X,$ grade output from $h)$, **Note too much on the topic yet\n",
    "\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/basic-learning-types.png\" />\n",
    "\n",
    "---\n",
    "### Math background\n",
    "##### Vector transpose ($w^T$) [link](https://www.khanacademy.org/math/linear-algebra/matrix-transformations/matrix-transpose/v/linear-algebra-transpose-of-a-vector)\n",
    "vector w transpose times vector x equal to number y ($w^Tx$ = y)\n",
    "\n",
    "#### Understanding the Dot Product [link](http://www.mvps.org/DirectX/articles/math/dot/index.htm) [khan proof](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/linear-algebra-vector-triangle-inequality)\n",
    "Help you to understand $w^Tx$ > 0 or $w^Tx$ < 0\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lecture 2 Is Learning Feasible?\n",
    "\n",
    "### 1. Learning the target function is impossible\n",
    "Meaning unknow target function f\n",
    "### 2. Hoeffding's Inequality\n",
    "$\\mathit{E}_{in}(h)$ means in-sample error. $\\mathit{E}_out(h)$ means out-sample error.\n",
    "\n",
    "Since we don't know the target function f, we cannot know the error rate for g in out-sample, but with hoeffding's inequality, we can bound $\\mathit{E}_in(g)$ probably close to $\\mathit{E}_out(g)$ greater than $\\epsilon$ to a small probability by giving big sample set N and proper $\\epsilon$\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/hoeffdings-inequality.png\" />\n",
    "\n",
    "We are trying to <b>learn</b> g$\\approx$f, means we are not only dealing with one hypothesis function h, but a set of them, H. Hoeffding's inequality only work for single case, if we apply it multi-times the probability of miss represent will greater than $\\epsilon$, so we need union bound for all h in H. M here is the number of hyphothesis in H. <b>M is determined by the learning algorithm model, bigger M looser the link between $\\mathit{E}_in(g)$ and $\\mathit{E}_out(g)$, means g might not able to generalize f</b>\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/the-final-verdict.png\" />\n",
    "### 3. What Does Learning Is Feasible Mean?\n",
    "1. Learning means base on training data to generalize. By giving trainning data N big enough, number of hypothesis M small enough and $\\epsilon$ reasonable enough, In-sample error can track out-sample $|\\mathit{E}_in(g) - \\mathit{E}_out(g)| < \\epsilon$, then we would know that our final hypothesis g can generalize.\n",
    "2. Learning means learning algorithm is able to pick one best g from H that has small in-simple error, and small out-simple error.\n",
    "\n",
    "---\n",
    "### Math background\n",
    "##### Hoeffding's Inequality\n",
    "This inequality is very important to machine learning, but I don't think we need to know the proof, I looked Wiki, the proof is complicated for me to understand, so I decide to move on and just going to understand the idea it is used in machine learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lecture 3 The Linear Model 1\n",
    "### 1. Input Representation\n",
    "Abstruct features is essensial for learning algorithm to create g, useless features will be noises and learning algorithm will fail to generalize.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/input-representation.png\" />\n",
    "\n",
    "### 2. The 'Pocket' Learning Algorithm\n",
    "It's a variation of PLA, it will keep the best hypothesis which has small in-sample error as the final hypothesis for Linear non-separable data.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/pocket-algorithm.png\" />\n",
    "\n",
    "### 3. Measure In-Sample Error For Real-Vauled Output\n",
    "We was talking about in-sample error $\\mathit{E}_in(h)$, but how to measure it? use $(h(x) - f(x))^2$, so called squared error, here x is a vector of all features, both outcomes from h(x) and f(x) are real-valued number. Since our h function is not only for 1 sample, so we need calculate the mean of squared error for all samples. N is number of samples, X is all samples, y is the vector of known outcomes from unknow target function f. w is the vector of weights for features.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/measure-the-error.png\" />\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/measure-the-error2.png\" />\n",
    "\n",
    "### 4. Minimizing $\\mathit{E}_in(h)$\n",
    "We are trying to minimizing the error of h, since h(X) = $Xw$ and we don't have control over X, so we need use linear regression to find out best weights vector w for all sample X to produce outcome close to f(X). Here we are using matrix calculus to get all the minizied value, I think it's very similar to multi-variable calculus, so I just provide the cheatsheet in mathbackground, we just need to know the idea that by calculate $\\frac{dh}{d\\vec{\\mathbf{w}}} = 0$, we get minimun error value.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/minimizing-error.png\" />\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/linear-regression.png\" />\n",
    "\n",
    "### 5. Linear Regression Classification\n",
    "We can use linear regression to calculate initial weights for binary classification.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/linear-regression-classification.png\" />\n",
    "\n",
    "### 6. Nonlinear Transformation\n",
    "Remember we don't have control over X, the only variable we can manipulate is the vector w in h(X). so we can treat X as constant, and do some nonlinear transformation, this won't change the linearity of w,then apply linear regression to w.\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/nonlinear-ransformation.png\" />\n",
    "\n",
    "\n",
    "### Summary\n",
    "<img width=\"500px\" height=\"500px\" src=\"./resources/the-linear-model-1-summary.png\" />\n",
    "\n",
    "---\n",
    "### Math background\n",
    "##### Matrix calculus cheatsheet\n",
    "<img width=\"250px\" height=\"250px\" src=\"https://i.stack.imgur.com/apR2q.png\" />\n",
    "\n",
    "##### Linear regression [link](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/residuals-least-squares-rsquared/a/introduction-to-residuals)<br>\n",
    "Great explaination on linear regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Terms\n",
    "[lecture3](#Section2)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
